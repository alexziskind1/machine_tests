{
  "schema_version": 1,
  
  "_comment_hardware_section": "Physical machine characteristics",
  "hardware": {
    "make": "Apple",
    "device_model": "MacBook Pro",
    "code": "mbp_m4max",
    "cpu": "M4 Max",
    "memory_gb": 128,
    "is_igpu": true,
    "gpu": "M4 Max",
    "vram_allocation": "dynamic",
    "vram_gb": 128,
    "notes": "Apple MacBook Pro with M4 Max (2024)"
  },
  
  "environment": {
    "os": "macOS Sequoia"
  },
  
  "_comment_target_section": "llama-bench executable and model configuration",
  "target": {
    "llama_bench_path": "~/Code/ml/llama.cpp/build/bin/llama-bench",
    "model_path": "/Users/alex/.cache/lm-studio/models/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf",
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-Q4_K_M",
    "quant": "Q4_K_M",
    "backend": "Metal",
    "threads": 12
  },
  
  "_comment_benchmark_section": "Benchmark parameters",
  "benchmark": {
    "prompt_sizes": [128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768],
    "output_tokens": 512,
    "iterations": 5,
    "warmup_iterations": 2
  },
  
  "_comment_results": "Results are saved to results/<model_name>/<quant>/<hardware_code>/"
}
