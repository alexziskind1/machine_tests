{
  "schema_version": 1,
  
  "_comment_hardware_section": "Physical machine characteristics",
  "hardware": {
    "make": "Apple",
    "device_model": "MacBook Pro",
    "code": "mbp_m5",
    "cpu": "M5",
    "memory_gb": 32,
    "is_igpu": true,
    "gpu": "M5",
    "vram_allocation": "dynamic",
    "vram_gb": 32,
    "notes": "Apple MacBook Pro with M5 (2025)"
  },
  
  "environment": {
    "os": "macOS Tahoe"
  },
  
  "_comment_target_section": "llama-bench executable and model configuration",
  "target": {
    "llama_bench_path": "~/Code/ml/llama.cpp/build/bin/llama-bench",
    "model_path": "/Users/alex/.cache/lm-studio/models/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf",
    "model_name": "Qwen3-Coder-30B-A3B-Instruct-Q4_K_M",
    "quant": "Q4_K_M",
    "backend": "Metal",
    "threads": 12
  },
  
  "_comment_benchmark_section": "Benchmark parameters",
  "benchmark": {
    "prompt_sizes": [128, 256, 512, 1024, 2048, 4096, 8192, 16384],
    "output_tokens": 512,
    "iterations": 5,
    "warmup_iterations": 2
  },
  
  "_comment_results": "Results are saved to results/<model_name>/<quant>/<hardware_code>/"
}
